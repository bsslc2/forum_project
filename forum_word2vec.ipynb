{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "from nltk import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./forum280.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [line.replace(\"\\n\",'') for line in open('stopwords.txt','r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutting(text):\n",
    "    text_c = jieba.cut(text)\n",
    "    return text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwords(w):\n",
    "    if w not in stop_tokens:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regular(w):\n",
    "    line = re.findall('[\\u4e00-\\u9fa5]+', w)\n",
    "    if len(line) > 0:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn_dict = {}\n",
    "with open(\"syn.txt\",\"r\") as f :\n",
    "    for line in f:\n",
    "        for word in line.strip(\"\\n\").split(\"\\t\")[1:]:\n",
    "            syn_dict[word] = line.strip(\"\\n\").split(\"\\t\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def syn(w):\n",
    "    if w in syn_dict.keys():\n",
    "        w=syn_dict[w]\n",
    "        return w\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version3 cut first, not what i want\n",
    "def trimming(data):\n",
    "    filtered_w = []\n",
    "    words = cutting(data) \n",
    "    for word in words:\n",
    "        word = regular(word)\n",
    "        if word is not None:\n",
    "            stop_w = stopwords(word[0])\n",
    "            if stop_w is not None:\n",
    "                syn_w = syn(stop_w)\n",
    "                filtered_w.append(syn_w)\n",
    "    return \" \".join(filtered_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Cathy\\Documents\\ETL\\myPractice\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\Cathy\\AppData\\Local\\Temp\\jieba.ub90dac2fb499a8444ce8934f4c359f32.cache\n",
      "Loading model cost 4.176 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data[\"tokens\"] = data['content'].map(trimming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1271"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((data.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['tokens'])\n",
    "# 將一行一行的文件變成 \"一個\" list\n",
    "# 可以當成英文字的概念就可以用tokenizer 處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "sentences = [get_tokens(r) for r in list(data['tokens'])]\n",
    "model = gensim.models.word2vec.Word2Vec(sentences, min_count=1, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('執政黨', 0.9999073147773743),\n",
       " ('免費', 0.9998955726623535),\n",
       " ('不同意', 0.999894380569458),\n",
       " ('說謊', 0.9998858571052551),\n",
       " ('走到', 0.9998411536216736),\n",
       " ('權益', 0.9998201131820679),\n",
       " ('極度', 0.9998135566711426),\n",
       " ('更多', 0.9997791051864624),\n",
       " ('負擔', 0.9997605085372925),\n",
       " ('重', 0.9997496604919434)]"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('悠遊卡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['中國','經濟' ], negative=['台灣'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cathy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('境外', 0.9998736381530762),\n",
       " ('就職', 0.9997354745864868),\n",
       " ('執照', 0.9994748830795288),\n",
       " ('和平', 0.9993702173233032),\n",
       " ('商店', 0.9992742538452148),\n",
       " ('物', 0.999163031578064),\n",
       " ('一貫', 0.9991096258163452),\n",
       " ('海運', 0.9990823268890381),\n",
       " ('通信', 0.9990628957748413),\n",
       " ('新南向', 0.9990330934524536)]"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('交流')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
